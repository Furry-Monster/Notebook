{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "\n",
      "\n",
      "tensor([[ 0,  4,  8, 12, 16],\n",
      "        [ 1,  5,  9, 13, 17],\n",
      "        [ 2,  6, 10, 14, 18],\n",
      "        [ 3,  7, 11, 15, 19]])\n",
      "\n",
      "\n",
      "tensor([[  14,   38,   62,   86,  110],\n",
      "        [  38,  126,  214,  302,  390],\n",
      "        [  62,  214,  366,  518,  670],\n",
      "        [  86,  302,  518,  734,  950],\n",
      "        [ 110,  390,  670,  950, 1230]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5,4)\n",
    "print(A)\n",
    "print('\\n')\n",
    "\n",
    "B = A.T\n",
    "print(B)\n",
    "print('\\n')\n",
    "\n",
    "C = A @ B\n",
    "print(C)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 4, 5],\n",
      "        [3, 5, 6]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "symmetric_matrix = torch.tensor([[1,2,3],[2,4,5],[3,5,6]])\n",
    "print(symmetric_matrix)\n",
    "\n",
    "print(symmetric_matrix.t() == symmetric_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管单个向量的默认方向是$列向量$，但在表示表格数据集的矩阵中，<br>\n",
    "将每个数据样本作为矩阵中的$行向量$更为常见。<br>\n",
    "后面的章节将讲到这点，这种约定将支持常见的深度学习实践。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "B = A.clone()\n",
    "\n",
    "A,A+B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体而言，[**两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号$\\odot$）**]。\n",
    "对于矩阵$\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$，\n",
    "其中第$i$行和第$j$列的元素是$b_{ij}$。\n",
    "矩阵$\\mathbf{A}$（在 :eqref:`eq_matrix_def`中定义）和$\\mathbf{B}$的Hadamard积为：\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.]],\n",
       " \n",
       "         [[12., 13., 14., 15.],\n",
       "          [16., 17., 18., 19.],\n",
       "          [20., 21., 22., 23.]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "A = torch.arange(24,dtype=torch.float32).reshape(2,3,4)\n",
    "A,A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Dimensionality Reduction(降维)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]]) \n",
      "\n",
      " torch.Size([2, 2, 3]) \n",
      "\n",
      " tensor(66) \n",
      "\n",
      "\n",
      "tensor([[ 6,  8, 10],\n",
      "        [12, 14, 16]]) \n",
      "\n",
      " torch.Size([2, 3]) \n",
      "\n",
      "\n",
      "tensor([[ 3,  5,  7],\n",
      "        [15, 17, 19]]) \n",
      "\n",
      " torch.Size([2, 3]) \n",
      "\n",
      "\n",
      "tensor([[ 3, 12],\n",
      "        [21, 30]]) \n",
      "\n",
      " torch.Size([2, 2]) \n",
      "\n",
      "\n",
      "tensor([18, 22, 26]) \n",
      "\n",
      " torch.Size([3]) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(2,2,3)\n",
    "print(x,'\\n\\n',x.shape,'\\n\\n',x.sum(),'\\n\\n')\n",
    "\n",
    "x_sum_axis0 = x.sum(axis=0)\n",
    "print(x_sum_axis0,'\\n\\n',x_sum_axis0.shape,'\\n\\n')\n",
    "\n",
    "x_sum_axis1 = x.sum(axis=1)\n",
    "print(x_sum_axis1,'\\n\\n',x_sum_axis1.shape,'\\n\\n')\n",
    "\n",
    "x_sum_axis2 = x.sum(axis=2)\n",
    "print(x_sum_axis2,'\\n\\n',x_sum_axis2.shape,'\\n\\n')\n",
    "\n",
    "x_sum_axis01 = x.sum(axis=(0,1))\n",
    "print(x_sum_axis01,'\\n\\n',x_sum_axis01.shape,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]]) \n",
      "\n",
      " torch.Size([2, 2, 3]) \n",
      "\n",
      " tensor(66) \n",
      "\n",
      "\n",
      "tensor([[[ 6,  8, 10],\n",
      "         [12, 14, 16]]])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  8, 10],\n",
      "         [12, 14, 16]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(2,2,3)\n",
    "print(x,'\\n\\n',x.shape,'\\n\\n',x.sum(),'\\n\\n')\n",
    "\n",
    "# get sum without reducing the dimension\n",
    "sum_x = x.sum(axis=0, keepdims = True)\n",
    "print(sum_x)\n",
    "\n",
    "sum_x = x.cumsum(axis=0)\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(7.)   tensor(7.)\n",
      "tensor(5.4772)\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "\n",
    "# L2 norm of a vector\n",
    "# norm calculation must be done on a tensor with float or double data type\n",
    "print(torch.norm(u))\n",
    "\n",
    "# L1 norm of a vector\n",
    "print(torch.norm(u, 1),' ',torch.sum(torch.abs(u)))\n",
    "\n",
    "# Frobenius norm of a matrix\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(torch.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 范数和目标\n",
    "\n",
    "在深度学习中，我们经常试图解决优化问题：\n",
    "*最大化*分配给观测数据的概率;\n",
    "*最小化*预测和真实观测之间的距离。\n",
    "用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。\n",
    "目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。\n",
    "\n",
    "\n",
    "## 小结(Linear Algebra Review)\n",
    "\n",
    "* 标量、向量、矩阵和张量是线性代数中的基本数学对象。\n",
    "* 向量泛化自标量，矩阵泛化自向量。\n",
    "* 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。\n",
    "* 一个张量可以通过`sum`和`mean`沿指定的轴降低维度。\n",
    "* 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。\n",
    "* 在深度学习中，我们经常使用范数，如$L_1$范数、$L_2$范数和Frobenius范数。\n",
    "* 我们可以对标量、向量、矩阵和张量执行各种操作。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
